{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Lorena Barbosa Antunes Da Silva\n",
    "\n",
    "Nome: Jos√© Edson Mendon√ßa Ribeiro Lima Ara√∫jo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\loren\\anaconda3\\lib\\site-packages (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "!pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diret√≥rio\n",
      "C:\\Users\\loren\\Repositorios\\CDados_Projeto1\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diret√≥rio')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e n√£o relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Peaky Blinders.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>peaky fucking blinders foi a melhor coisa que ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@limawt1 peaky blinders est√° vindo em breve!! ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gente cancela ele me seguiu de outro instagram...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at√© eu que n√£o sou fumante fico com vontade de...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tou a curtir bue de peaky blinders</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento  Classifica√ß√£o\n",
       "0  peaky fucking blinders foi a melhor coisa que ...            1.0\n",
       "1  @limawt1 peaky blinders est√° vindo em breve!! ...            0.0\n",
       "2  gente cancela ele me seguiu de outro instagram...            0.0\n",
       "3  at√© eu que n√£o sou fumante fico com vontade de...            0.0\n",
       "4                 tou a curtir bue de peaky blinders            1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel(filename)\n",
    "linha=np.arange(0,300,1)\n",
    "train=train.iloc[linha,[0,1]]\n",
    "train.head(5)\n",
    "#train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>viciada em peaky blinders üñ§</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>@pedropossobom peaky blinders mastermind \\nmei...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>@italoverdose peaky blinders est√° vindo em bre...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>a gente fuma tbem, somos os peaky blinders htt...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>o lado bom de t√° com covid e que eu t√¥ maraton...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Teste  Classifica√ß√£o\n",
       "195                        viciada em peaky blinders üñ§            1.0\n",
       "196  @pedropossobom peaky blinders mastermind \\nmei...            1.0\n",
       "197  @italoverdose peaky blinders est√° vindo em bre...            0.0\n",
       "198  a gente fuma tbem, somos os peaky blinders htt...            0.0\n",
       "199  o lado bom de t√° com covid e que eu t√¥ maraton...            1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "linha=np.arange(0,200,1)\n",
    "test=test.iloc[linha,[0,1]]\n",
    "test.head(5)\n",
    "test.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador autom√°tico de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fa√ßa aqui uma descri√ß√£o do seu produto e o que considerou como relevante ou n√£o relevante na classifica√ß√£o dos tweets.\n",
    "\n",
    "ESCREVA AQUI...\n",
    "\n",
    "Produto: S√©rie Peaky Blinders, consideramos relevante qualquer coment√°rio positivo ou negativo, relacionados a s√©rie mostrando que est√£o assistindo ou indicando. Exemplo: \"N√£o curti assistir Peaky Blinders\", \"@user1 vc precisa assistir peaky blinders\", \"acabei de maratonar essa serie peaky blinders\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.options.display.max_rows = 13\n",
    "\n",
    "\n",
    "import re \n",
    "\n",
    "\n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Fun√ß√£o de limpeza muito simples que troca alguns sinais b√°sicos por espa√ßos\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    limpa_emoji=' '.join(emoji.get_emoji_regexp().split(text))\n",
    "    punctuation = '[!-.:?;]' # Note que os sinais [] s√£o delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', limpa_emoji)\n",
    "    \n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comentarios_relevantes=[]\n",
    "comentarios_irrelevantes=[]\n",
    "for i in range(0,300,1):\n",
    "    comentario=cleanup(train.Treinamento[i]).lower().split()\n",
    "    if train.Classifica√ß√£o[i]==1:\n",
    "        comentarios_relevantes.append(comentario)\n",
    "    elif train.Classifica√ß√£o[i]==0:\n",
    "        comentarios_irrelevantes.append(comentario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_irrelevantes=[]\n",
    "l_relevantes=[]\n",
    "for comentario in comentarios_irrelevantes:\n",
    "    lista_remocao=[]\n",
    "    for palavra in comentario:\n",
    "        if  palavra[0]=='@' or 'htt' == palavra[0:3]:\n",
    "            lista_remocao.append(palavra)\n",
    "    for palavra in lista_remocao:\n",
    "        comentario.remove(palavra)\n",
    "    for palavra in comentario:\n",
    "        l_irrelevantes.append(palavra)\n",
    "        \n",
    "for comentario in comentarios_relevantes:\n",
    "    lista_remocao=[]\n",
    "    for palavra in comentario:\n",
    "        if  palavra[0]=='@' or 'htt' == palavra[0:3]:\n",
    "            lista_remocao.append(palavra)\n",
    "    for palavra in lista_remocao:\n",
    "        comentario.remove(palavra)\n",
    "\n",
    "for comentario in comentarios_irrelevantes:\n",
    "    for palavra in comentario:\n",
    "        l_irrelevantes.append(palavra)\n",
    "for comentario in comentarios_relevantes:\n",
    "    for palavra in comentario:\n",
    "        l_relevantes.append(palavra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_l_relevantes=  pd.Series(l_relevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_l_irrelevantes=  pd.Series(l_irrelevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_relevantes = s_l_relevantes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_irrelevantes = s_l_irrelevantes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = comentarios_relevantes + comentarios_irrelevantes\n",
    "lista_total = l_irrelevantes + l_relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_l_total = pd.Series(lista_total)\n",
    "tab_l_total= s_l_total.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora voc√™ deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_relevantes=[]\n",
    "test_irrelevantes=[]\n",
    "test_total = []\n",
    "\n",
    "for i in range(0,200,1):\n",
    "    comentarios_test=cleanup(test.Teste[i]).lower().split()\n",
    "    \n",
    "    if test.Classifica√ß√£o[i]==1:\n",
    "        test_relevantes.append(comentarios_test)\n",
    "        \n",
    "    elif test.Classifica√ß√£o[i]==0:\n",
    "        test_irrelevantes.append(comentarios_test)\n",
    "        \n",
    "    test_total.append(comentarios_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_relevante = len(l_relevantes)/len(lista_total)\n",
    "P_irrelevante = len(l_irrelevantes)/len(lista_total)\n",
    "P_relevante + P_irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_relevante_dado_comentario = []\n",
    "P_irrelevante_dado_comentario = []\n",
    "relevancia_classificador = []\n",
    "i=0\n",
    "\n",
    "while i < len(test_total): \n",
    "    P_comentario_dado_relevante = 1\n",
    "    P_comentario_dado_irrelevante = 1\n",
    "   \n",
    "    for palavra in test_total[i]:\n",
    "        if palavra not in l_relevantes:\n",
    "            tab_relevantes[palavra]=0\n",
    "            \n",
    "        if palavra not in l_irrelevantes:\n",
    "            tab_irrelevantes[palavra]=0\n",
    "            \n",
    "        P_comentario_dado_relevante *= (tab_relevantes[palavra]+1)/(len(l_relevantes)+tab_l_total.size)\n",
    "        P_comentario_dado_irrelevante *= (tab_irrelevantes[palavra]+1)/(len(l_irrelevantes)+tab_l_total.size)\n",
    "    \n",
    "    P_relevante_dado_comentario.append(P_comentario_dado_relevante*P_relevante)\n",
    "    P_irrelevante_dado_comentario.append(P_comentario_dado_irrelevante*P_irrelevante)\n",
    "    \n",
    "    if P_relevante_dado_comentario[i] > P_irrelevante_dado_comentario[i]:\n",
    "        relevancia_classificador.append(1)\n",
    "        \n",
    "    else:\n",
    "        relevancia_classificador.append(0)\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Relevancia_Classificador\"]=relevancia_classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Relevancia_Classificador</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifica√ß√£o</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>20.5</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Relevancia_Classificador     0     1\n",
       "Classifica√ß√£o                       \n",
       "0.0                       20.5  34.5\n",
       "1.0                        3.0  42.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Matriz_de_Compara√ß√£o = pd.crosstab(test[\"Classifica√ß√£o\"] , test[\"Relevancia_Classificador\"], normalize=True)*100\n",
    "Matriz_de_Compara√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia:62.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"Acur√°cia:{0:.2f}%\".format(Matriz_de_Compara√ß√£o[0][0] + Matriz_de_Compara√ß√£o[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso classificador teve uma **acur√°cia de 62,50%**, o que significa que ele classifica de forma adequada **62,50%** dos tweets analisados.\n",
    "* Observou-se um percentual de **42,00% verdadeiros positivos**, ou seja, aproximadamente **42,00%** dos coment√°rios relevantes foram corretamente classificados como relevantes. \n",
    "* Observou-se um percentual de **34,50% falsos positivos**, ou seja, aproximadamente **34,50%** dos coment√°rios irrelevantes foram incorretamente classificados como relevantes. \n",
    "* Observou-se um percentual de **20,50% verdadeiros negativos**, ou seja, aproximadamente **20,50%** dos coment√°rios irrelevantes foram corretamente classificados como irrelevantes. \n",
    "* Observou-se um percentual de **3,00% falsos negativos**, ou seja, aproximadamente **3,00%** dos coment√°rios relevantes foram incorretamente classificados como irrelevantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sarcasmo e dupla nega√ß√£o\n",
    "\n",
    "Neste projeto, com o classificador Na√Øve-Bayes, assumiu-se a independ√™ncia entre as palavras, considerando-se apenas a probabilidade destas de aparecerem em determinada categoria. Ou seja, o classificador n√£o leva em considera√ß√£o as rela√ß√µes entre palavras e a sintaxe da l√≠ngua portuguesa. O que pode levar √† uma classifica√ß√£o err√¥nea de frases sarc√°sticas, ou com dupla nega√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_sarcastica = \"Peaky Binders √© t√£o bom quanto tomar chuva\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_duplanegacao = \"Eu nunca n√£o vou assistir Peaky Blinders\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plano de Expans√£o\n",
    "\n",
    "Tendo em considere√ß√£o que o projeto j√° apresentou 62,5% de acur√°cia. Para que o classificador fosse mais preciso poder-se-iam fazer futuras itera√ß√µes que justificariam um maior investimento no projeto, como uma limpeza adicional dos tweets obtidos; levar em conta a sintaxe e organiza√ß√£o entre palavras da l√≠ngua portuguesa, ou seja, n√£o admitir que as palavras t√™m probabilidade de aparecer em uma frase independentemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item6(repeticoes):\n",
    "    train_dados6 = pd.read_excel(filename)\n",
    "    linha=np.arange(0,300,1)\n",
    "    train_dados6=train_dados6.iloc[linha,[0,1]]\n",
    "    test_dados6 = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "    linha=np.arange(0,200,1)\n",
    "    test_dados6=test_dados6.iloc[linha,[0,1]]\n",
    "    for contador_item6 in range(0, repeticoes):\n",
    "        test_dados6 = test_dados6.rename(columns={\"Teste\":\"Treinamento\"})\n",
    "        total=pd.concat([train_dados6,test_dados6], ignore_index = True)\n",
    "        treino_dados = total.sample(300)\n",
    "        teste_dados = total.drop(treino_dados.index)\n",
    "        teste_dados = teste_dados.rename(columns = {\"Treinamento\":\"Teste\"})\n",
    "\n",
    "        \n",
    "        relevantes_treino6=[]\n",
    "        irrelevantes_treino6=[]\n",
    "        for i in range(0,300,1):\n",
    "            comentario6_treino=cleanup(treino_dados.Treinamento[i]).lower().split()\n",
    "            if treino_dados.Classifica√ß√£o[i]==1:\n",
    "                relevantes_treino6.append(comentario6_treino)\n",
    "            elif treino_dados.Classifica√ß√£o[i]==0:\n",
    "                irrelevantes_treino6.append(comentario6_treino)\n",
    "        \n",
    "        relevantes_teste6=[]\n",
    "        irrelevantes_teste6 = []\n",
    "        total_teste_6 = []\n",
    "        for j in range(0,200,1):\n",
    "            comentario6_teste=cleanup(teste_dados.Teste[j]).lower().split()\n",
    "            if teste_dados.Classifica√ß√£o[j]==1:\n",
    "                relevantes_teste6.append(comentario6_teste)\n",
    "            elif teste_dados.Classifica√ß√£o[j]==0:\n",
    "                irrelevantes_teste6.append(comentario6_teste)\n",
    "                \n",
    "            total_teste6.append(comentarios6_teste)\n",
    "                \n",
    "        l_irrelevantes_treino6=[]\n",
    "        l_relevantes_treino6=[]\n",
    "        for comentario in irrelevantes_treino6:\n",
    "            lista_remocao=[]\n",
    "        for palavra in comentario:\n",
    "            if  palavra[0]=='@' or 'htt' == palavra[0:3]:\n",
    "                lista_remocao.append(palavra)\n",
    "        for palavra in lista_remocao:\n",
    "            comentario.remove(palavra)\n",
    "        for palavra in comentario:\n",
    "            l_irrelevantes_treino6.append(palavra)\n",
    "        \n",
    "        for comentario in relevantes_treino6:\n",
    "            lista_remocao=[]\n",
    "        for palavra in comentario:\n",
    "            if  palavra[0]=='@' or 'htt' == palavra[0:3]:\n",
    "                lista_remocao.append(palavra)\n",
    "        for palavra in lista_remocao:\n",
    "            comentario.remove(palavra)\n",
    "\n",
    "        for comentario in irrelevantes_treino6:\n",
    "            for palavra in comentario:\n",
    "                l_irrelevantes_treino6.append(palavra)\n",
    "        for comentario in relevantes_treino6:\n",
    "            for palavra in comentario:\n",
    "                l_relevantes_treino6.append(palavra)\n",
    "                \n",
    "        s_l_relevantes_treino6=  pd.Series(l_relevantes_treino6)\n",
    "        s_l_irrelevantes_treino6=  pd.Series(l_irrelevantes_treino6)\n",
    "        \n",
    "        tab_relevantes_treino6 = s_l_relevantes_treino6.value_counts()\n",
    "        tab_irrelevantes_treino6 = s_l_irrelevantes_treino.value_counts()\n",
    "        \n",
    "        lista_total = l_irrelevantes_treino6 + l_relevantes_treino6\n",
    "        \n",
    "        s_l_total = pd.Series(lista_total)\n",
    "        tab_l_total= s_l_total.value_counts()\n",
    "        \n",
    "        \n",
    "        P_relevante6 = len(l_relevantes_treino6)/len(lista_total)\n",
    "        P_irrelevante6 = len(l_irrelevantes_treino6)/len(lista_total)\n",
    "        \n",
    "        P_relevante_dado_comentario6 = []\n",
    "        P_irrelevante_dado_comentario6 = []\n",
    "        relevancia_classificador6 = []\n",
    "        ki=0\n",
    "\n",
    "        while k < len(total_teste6): \n",
    "            P_comentario_dado_relevante6 = 1\n",
    "            P_comentario_dado_irrelevante6 = 1\n",
    "   \n",
    "            for palavra in total_teste6[k]:\n",
    "                if palavra not in l_relevantes_treino6:\n",
    "                    tab_relevantes_treino6[palavra]=0\n",
    "            \n",
    "                if palavra not in l_irrelevantes_treino6:\n",
    "                    tab_irrelevantes_treino6[palavra]=0\n",
    "            \n",
    "                P_comentario_dado_relevante6 *= (tab_relevantes_treino6[palavra]+1)/(len(l_relevantes_treino6)+tab_l_total.size)\n",
    "                P_comentario_dado_irrelevante6 *= (tab_irrelevantes_treino6[palavra]+1)/(len(l_irrelevantes_treino6)+tab_l_total.size)\n",
    "    \n",
    "            P_relevante_dado_comentario6.append(P_comentario_dado_relevante6*P_relevante6)\n",
    "            P_irrelevante_dado_comentario6.append(P_comentario_dado_irrelevante6*P_irrelevante6)\n",
    "    \n",
    "            if P_relevante_dado_comentario6[k] > P_irrelevante_dado_comentario6[k]:\n",
    "                relevancia_classificador.append(1)\n",
    "        \n",
    "            else:\n",
    "                relevancia_classificador.append(0)\n",
    "        \n",
    "            k += 1\n",
    "            \n",
    "            teste_dados[\"Relevancia_Classificador\"]=relevancia_classificador\n",
    "            Matriz_de_Compara√ß√£o6 = pd.crosstab(teste_dados[\"Classifica√ß√£o\"] , teste_dados[\"Relevancia_Classificador\"], normalize=True)*100\n",
    "            \n",
    "            acertos[contador_item6] = Matriz_de_Compara√ß√£o6[0][0]+Matriz_de_Compara√ß√£o6[1][1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-88a9528b6de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mitem6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-121-02f84ffac4b7>\u001b[0m in \u001b[0;36mitem6\u001b[1;34m(repeticoes)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mirrelevantes_treino6\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mcomentario6_treino\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreino_dados\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTreinamento\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtreino_dados\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClassifica√ß√£o\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mrelevantes_treino6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomentario6_treino\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "item6(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfei√ßoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B v√£o evoluir em conceito dependendo da quantidade de itens avan√ßados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transforma√ß√µes que n√£o afetem a qualidade da informa√ß√£o contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CORRIGIU separa√ß√£o de espa√ßos entre palavras e emojis ou entre emojis e emojis\n",
    "* CRIOU categorias intermedi√°rias de relev√¢ncia baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante. Pelo menos quatro categorias, com adi√ß√£o de mais tweets na base, conforme enunciado. (OBRIGAT√ìRIO PARA TRIOS, sem contar como item avan√ßado)\n",
    "* EXPLICOU porqu√™ n√£o pode usar o pr√≥prio classificador para gerar mais amostras de treinamento\n",
    "* PROP√îS diferentes cen√°rios para Na√Øve Bayes fora do contexto do projeto\n",
    "* SUGERIU e EXPLICOU melhorias reais com indica√ß√µes concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item 6. Qualidade do Classificador a partir de novas separa√ß√µes dos tweets entre Treinamento e Teste descrito no enunciado do projeto (OBRIGAT√ìRIO para conceitos A ou A+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Refer√™ncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
